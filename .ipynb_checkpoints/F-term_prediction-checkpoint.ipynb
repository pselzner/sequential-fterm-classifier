{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69290130-c1f8-4ccc-a4d8-c77e831f374e",
   "metadata": {},
   "source": [
    "# Sequential Prediction of F-Terms\n",
    "\n",
    "This notebook provides some code to load the model from huggingface and predict f-terms for given patent abstracts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89062a2d-36fb-46aa-8b59-f86d0e482e31",
   "metadata": {},
   "source": [
    "## load model and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a43ba5-2342-48f1-9e77-6634496020ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -r requirements.txt\n",
    "from transformers import AutoTokenizer, OPTForCausalLM\n",
    "import torch\n",
    "import pickle as pk\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf055a-8055-425d-8c8e-6c79180de36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Request access via https://huggingface.co/RWTH-TIME/galactica-125m-f-term-classification\n",
    "default_dtype = torch.bfloat16\n",
    "torch.set_default_dtype(default_dtype)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"RWTH-TIME/galactica-125m-f-term-classification\")\n",
    "model = OPTForCausalLM.from_pretrained(\"RWTH-TIME/galactica-125m-f-term-classification\", torch_dtype=default_dtype, low_cpu_mem_usage=True,\n",
    "                                           device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a34587-279b-4afd-b62a-14ca57247bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load theme, viewpoint and f-term dictionaries from pickle files\n",
    "\"\"\"\n",
    "with open(f'data/full_descriptions.pk', 'rb') as f:\n",
    "    full_descriptions_dict = pk.load(f)\n",
    "\"\"\"\n",
    "\n",
    "with open(f'data/f_term_dict.pk', 'rb') as f:\n",
    "    f_term_dict = pk.load(f)\n",
    "\n",
    "with open(f'data/themes_descriptions.pk', 'rb') as f:\n",
    "    themes_descriptions = pk.load(f)\n",
    "\n",
    "with open(f'data/viewpoints_descriptions.pk', 'rb') as f:\n",
    "    viewpoints_descriptions = pk.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c2653e-3c0a-4107-92ec-a45303b00503",
   "metadata": {},
   "source": [
    "## predict F-terms for given abstracts or technological descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e24dc96-7855-4d91-8731-a3350ec42b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    prompt, \n",
    "    model, \n",
    "    tokenizer, \n",
    "    max_pred_tokens=10, \n",
    "    decode=True, \n",
    "    enforce_no_repetition=True, \n",
    "    ignore_eos_token=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates FTERM classifications for a given patent abstract.\n",
    "\n",
    "    Prompts a given model and returns comma-separated FTERMS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prompt : str\n",
    "        A patent abstract text or technology description.\n",
    "    model : transformers.models.opt.modeling_opt.OPTForCausalLM\n",
    "        The transformer model used for classification.\n",
    "    tokenizer : transformers.PreTrainedTokenizer\n",
    "        The tokenizer associated with the model.\n",
    "    max_pred_tokens : int, optional\n",
    "        The maximum number of patent classes to predict (default is 10).\n",
    "    decode : bool, optional\n",
    "        If True, outputs decoded text classes; otherwise, returns token IDs (default is True).\n",
    "    enforce_no_repetition : bool, optional\n",
    "        If True, inhibits repeated prediction of the same FTERM class (default is True).\n",
    "    ignore_eos_token : bool, optional\n",
    "        If True, enforces prediction of max_pred_tokens and ignores the model's EOS token (default is True).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or list of int\n",
    "        A list of FTERM classes for the given prompt, either decoded or as token IDs.\n",
    "    \"\"\"\n",
    "    # Add the start FTERM token to the prompt\n",
    "    prompt += \"<START F-TERMS>\"\n",
    "\n",
    "    # Convert the prompt to tokens\n",
    "    eos_token_id = -999 if ignore_eos_token else tokenizer.eos_token_id\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    prompt_tokens = tokenized[\"input_ids\"][:, :-1]\n",
    "    attention_mask = tokenized[\"attention_mask\"][:, :-1]\n",
    "\n",
    "    # Initialize variables for generation\n",
    "    predictions = []\n",
    "    current_token = -100\n",
    "\n",
    "    # Generate the FTERMS\n",
    "    while current_token != eos_token_id and len(predictions) < max_pred_tokens:\n",
    "        # Model forward pass\n",
    "        output = model(\n",
    "            input_ids=prompt_tokens, \n",
    "            attention_mask=attention_mask, \n",
    "            output_attentions=False, \n",
    "            output_hidden_states=False, \n",
    "            return_dict=True\n",
    "        )\n",
    "        logits = output[\"logits\"]\n",
    "\n",
    "        # Get token predictions sorted by likelihood\n",
    "        current_token_predictions = torch.sort(logits[0, -1], dim=-1, descending=True)\n",
    "        i = 0\n",
    "        current_token = current_token_predictions[1][i].item() + 50000\n",
    "\n",
    "        # Handle no repetition and EOS token rules\n",
    "        while (\n",
    "            (enforce_no_repetition and current_token in predictions) or \n",
    "            (current_token == tokenizer.eos_token_id and ignore_eos_token)\n",
    "        ):\n",
    "            i += 1\n",
    "            current_token = current_token_predictions[1][i].item() + 50000\n",
    "\n",
    "        predictions.append(current_token)\n",
    "\n",
    "        # Update prompt tokens and attention mask\n",
    "        prompt_tokens = torch.cat([prompt_tokens, torch.tensor([[current_token]])], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask[:, -1:]], dim=-1)\n",
    "\n",
    "    # Decode predictions if required\n",
    "    if decode:\n",
    "        return tokenizer.decode(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacaeabd-3076-40c9-b4a1-e58c73144131",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "abstract=\"PROBLEM TO BE SOLVED: To enable biological rhythm of small animals to be regulated. <P>SOLUTION: The cushion 1 for small animal includes a base part 2 on which the small animal can lay the body, a swelled part 3 formed on the base part 2, a light-radiating part 5 for irradiating light to the small animal lying on the base part 2, and a control part 8 for switching the light irradiated from the light-radiating part 5 according to the time regulated based on a previously set light pattern. For example, the light-irradiating part 5 includes a light source part 6 that includes a light source in the inside, and a light-inlet part 7 for introducing the light from the light source, while emitting light.\"\n",
    "generate(abstract, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91912b37-8df9-4a02-8ac3-96fbe97148ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Translating F-term predictions:\n",
    "output=generate(abstract, model, tokenizer)\n",
    "output=output.split(\",\")[:-1]\n",
    "for fterm in output:\n",
    "    print(f' Theme: {themes_descriptions[fterm[:5]]} | Viewpoint: {viewpoints_descriptions[fterm[:8]]} | F-term: {f_term_dict[fterm]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gittest",
   "language": "python",
   "name": "gittest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
